{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cart-Pole.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOGvSuKuPZHQqhN20qtrC5d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hujanais/reinforcement-learning/blob/master/Cart_Pole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8McXM4OOi7zC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Solving Cart-Pole with DQN."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZQkoLLgtllk",
        "colab_type": "code",
        "outputId": "b3630e14-d128-4494-fffa-1a567ec2ef10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import output\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from matplotlib import style\n",
        "from collections import deque\n",
        "\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import gym\n",
        "\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8LGSgo9cz9-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cfdbf026-d57f-4f97-d0b3-ac8f63b83da5"
      },
      "source": [
        "# some google drive magic.\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#import os\n",
        "#os.chdir('models')\n",
        "#!ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cart-pole.h5  ttt_20k.h5  ttt.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cttkUJNkt-hY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# explore the environment to see what does this game do.\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "state_size = env.observation_space.shape[0]\n",
        "print(state_size)\n",
        "print(env.observation_space.low, env.observation_space.high, env.action_space.n)\n",
        "done = False\n",
        "state = env.reset()\n",
        "\n",
        "#  Type: Box(4)\n",
        "#         Num\tObservation               Min             Max\n",
        "#         0\tCart Position             -4.8            4.8\n",
        "#         1\tCart Velocity             -Inf            Inf\n",
        "#         2\tPole Angle                -24 deg         24 deg\n",
        "#         3\tPole Velocity At Tip      -Inf            Inf\n",
        "\n",
        "angles = []\n",
        "velocities = []\n",
        "total_reward = 0\n",
        "actions = []\n",
        "while not done:\n",
        "\n",
        "  angle = state[2]\n",
        "  if (angle < 0):\n",
        "    action = 0\n",
        "  else:\n",
        "    action = 1\n",
        "  actions.append(action)\n",
        "\n",
        "  next_state, reward, done, info = env.step(action)\n",
        "  total_reward += reward\n",
        "\n",
        "  angles.append(next_state[2])\n",
        "  velocities.append(next_state[3])\n",
        "\n",
        "  state = next_state\n",
        "\n",
        "print (total_reward)\n",
        "fig, axs = plt.subplots(3)\n",
        "fig.suptitle('Vertically stacked subplots')\n",
        "axs[0].plot(angles)\n",
        "axs[1].plot(actions)\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJVbkZhx8vJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The DQN Agent\n",
        "\n",
        "class Agent:\n",
        "  def __init__(self, state_size, action_size):\n",
        "\n",
        "    # setup a deque for experience replay.\n",
        "    self.memory = deque(maxlen=10000)\n",
        "    self.batch_size = 32\n",
        "\n",
        "    self.episode = 0\n",
        "\n",
        "    # setup the state and action space.\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "\n",
        "    # neural network hyperparameters.\n",
        "    self.learning_rate = 0.001\n",
        "\n",
        "    # hyperparameters for epsilon decay\n",
        "    self.gamma = 0.99  # discount rate\n",
        "    self.epsilon = 1.0 # exploration threshold\n",
        "    self.max_epsilon = 1.0\n",
        "    self.min_epsilon = 0.01\n",
        "    self.decay_rate = 0.0001 # exponential decay rate for explore-exploit\n",
        "\n",
        "    # create a policy network and a target network.\n",
        "    self.policy = self.__build_model()\n",
        "    self.target_model = self.__build_model()\n",
        "\n",
        "    self.target_update_cycles = 50  # the number of training after which to update the target network with the weights from the policy network\n",
        "\n",
        "  # get the action with epsilon-greedy. used for training.\n",
        "  # state is a one-hot-vector representation of the state.\n",
        "  def act(self, state):\n",
        "    exp_exp_tradeoff = random.uniform(0,1)\n",
        "    if (exp_exp_tradeoff > self.epsilon):\n",
        "      action = self.getGreedyAction(state) # exploit\n",
        "    else:\n",
        "      action = random.randrange(0, self.action_size)  # explore\n",
        "\n",
        "    return action\n",
        "\n",
        "  # get the greedy action\n",
        "  def getGreedyAction(self, state):\n",
        "    state_tensor = state.reshape(1, self.state_size)\n",
        "    act_values = self.policy.predict(state_tensor)\n",
        "    action = np.argmax(act_values[0])\n",
        "    return action\n",
        "\n",
        "  # exp => [state, action, reward, next_state, done]\n",
        "  def remember(self, state, action, reward, next_state, done):\n",
        "    self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "  # train the model using experience replay.\n",
        "  def train(self):\n",
        "    if (len(self.memory) >= self.batch_size):\n",
        "      minibatch = random.sample(self.memory, self.batch_size)\n",
        "      for state, action, reward, next_state, done in minibatch:\n",
        "        target = reward # this is the definition of the terminal state.\n",
        "        if not done:\n",
        "          next_state_tensor = next_state.reshape(1, self.state_size)\n",
        "          target = reward + self.gamma * np.amax(self.target_model.predict(next_state_tensor)[0])\n",
        "\n",
        "        state_tensor = state.reshape(1, self.state_size)\n",
        "        target_arr = self.policy.predict(state_tensor)\n",
        "\n",
        "        target_arr[0][action] = target\n",
        "        self.policy.fit(state_tensor, target_arr, verbose=0)\n",
        "\n",
        "    self.decayEpsilon()\n",
        "\n",
        "  # at the end of each training episode, decay the epsilon\n",
        "  # to slowly facilitate more exploitation over exploration\n",
        "  # We will also update the target network here.\n",
        "  def decayEpsilon(self):\n",
        "    self.episode += 1\n",
        "    self.epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon)*np.exp(-self.decay_rate*self.episode)\n",
        "\n",
        "    if (self.episode%self.target_update_cycles == 0):\n",
        "      self.target_model.set_weights(self.policy.get_weights()) \n",
        "\n",
        "  # convert int to one hot vector\n",
        "  def __to_onehot(self, value, size):\n",
        "    one_hot_vector = np.zeros(size)\n",
        "    one_hot_vector[value] = 1\n",
        "    return one_hot_vector\n",
        "\n",
        "  # build the neural network.\n",
        "  def __build_model(self):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(32, input_shape=(self.state_size,) , activation='relu')) # input_dim = 3 is equilavent with input_shape(3,)\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(self.action_size, activation='linear'))\n",
        "    model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do22yUV_-422",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the agent.\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "agent = Agent(4, 2)\n",
        "\n",
        "rewardsArr = []\n",
        "for episode in range(10000):\n",
        "  done = False\n",
        "  rewards = 0\n",
        "  state = env.reset()\n",
        "  while not done:\n",
        "    action = agent.act(state)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "\n",
        "    agent.remember(state, action, reward, next_state, done)\n",
        "    agent.train()\n",
        "\n",
        "    rewards += reward\n",
        "    state = next_state\n",
        "\n",
        "  if(episode != 0 and episode%20==0):\n",
        "    output.clear('status_text')\n",
        "    print('Episode: {}.  Current Mean Score: {}'.format(episode, np.mean(rewardsArr[-20:]), end='\\r'))\n",
        "  \n",
        "  # save the model once in a while\n",
        "  if (episode != 0 and episode%1000==0):\n",
        "    output.clear('status_text')\n",
        "    agent.policy.save('cart-pole.h5')\n",
        "\n",
        "  rewardsArr.append(rewards)\n",
        "\n",
        "env.close()\n",
        "\n",
        "plt.plot(rewardsArr)\n",
        "plt.title('reward')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('score')\n",
        "axs[0].plot(rewardsArr)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyX_A8utqCoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test the agent\n",
        "\n",
        "# env = gym.make('CartPole-v0')\n",
        "# load the pre-saved model\n",
        "model = load_model('cart-pole.h5')\n",
        "\n",
        "angles = []\n",
        "positions = []\n",
        "total_reward = 0\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "  state_tensor = state.reshape(1,4)\n",
        "  act_values = model.predict(state_tensor)\n",
        "  action = np.argmax(act_values[0])\n",
        "  next_state, reward, done, info = env.step(action)\n",
        "  done = True if reward == 0 else False\n",
        "  total_reward += reward\n",
        "\n",
        "  positions.append(next_state[0])\n",
        "  angles.append(next_state[2])\n",
        "\n",
        "  env.render()\n",
        "  state = next_state\n",
        "\n",
        "print ('Reward: {}'.format(total_reward))\n",
        "fig, axs = plt.subplots(2)\n",
        "fig.suptitle('Vertically stacked subplots')\n",
        "axs[0].plot(angles)\n",
        "axs[0].set_title('angle')\n",
        "axs[0].set_xlabel('epoch')\n",
        "\n",
        "axs[1].plot(positions)\n",
        "axs[1].set_title('cart-position')\n",
        "axs[1].set_xlabel('epoch')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBAX7wvbz56x",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}